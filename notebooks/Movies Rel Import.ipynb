{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7d941b-7901-4649-8d7a-c9e68d014685",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba01e3e7-20db-4d99-a68c-2efd12c424fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "Neo4jConnectionInfo = {\n",
    "    \"URI\": \"neo4j+s://62d77af8.databases.neo4j.io\", \n",
    "    \"Username\": dbutils.secrets.get(scope=\"neo4j\", key=\"username\"), \n",
    "    \"Password\": dbutils.secrets.get(scope=\"neo4j\", key=\"password\")\n",
    "}\n",
    "\n",
    "LoadSettings = {\n",
    "    \"csvDir\": \"file:/Workspace/Users/eric.monk@neo4j.com/movie_csvs/\",\n",
    "    \"writeMode\": \"Overwrite\",\n",
    "    \"partitionColName\": \"partitionCode\"\n",
    "}\n",
    "\n",
    "driver = GraphDatabase.driver(Neo4jConnectionInfo[\"URI\"], auth=(Neo4jConnectionInfo[\"Username\"], Neo4jConnectionInfo[\"Password\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a86960dc-1e08-480f-b036-0acc10ef9323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function\n",
    "def run(driver, query, params=None):\n",
    "    with driver.session() as session:\n",
    "        if params is not None:\n",
    "            return [r for r in session.run(query, params)]\n",
    "        else:\n",
    "            return [r for r in session.run(query)]\n",
    "\n",
    "run(driver,'CREATE CONSTRAINT movie_userId_unique IF NOT EXISTS FOR (user:MovieUser) REQUIRE user.userId  IS UNIQUE')\n",
    "\n",
    "run(driver,'CREATE CONSTRAINT movieId_unique IF NOT EXISTS FOR (movie:Movie) REQUIRE movie.movieImdbId IS UNIQUE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9afdf95c-fb3e-47e6-b453-45511fd591e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def writeTableToNeo (tableName, cypherQuery):\n",
    "    df = spark.read.table(tableName)\n",
    "    result = (df.write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", Neo4jConnectionInfo[\"URI\"])\n",
    "        .option(\"authentication.basic.username\", Neo4jConnectionInfo[\"Username\"])\n",
    "        .option(\"authentication.basic.password\", Neo4jConnectionInfo[\"Password\"])\n",
    "        .option(\"query\", cypherQuery)\n",
    "        .mode(LoadSettings[\"writeMode\"])\n",
    "        .save())\n",
    "    \n",
    "def writeTableToNeoSinglePartition (tableName, cypherQuery): \n",
    "    df = spark.read.table(tableName).repartition(1)\n",
    "    result = (df.write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", Neo4jConnectionInfo[\"URI\"])\n",
    "        .option(\"authentication.basic.username\", Neo4jConnectionInfo[\"Username\"])\n",
    "        .option(\"authentication.basic.password\", Neo4jConnectionInfo[\"Password\"])\n",
    "        .option(\"query\", cypherQuery)\n",
    "        .mode(LoadSettings[\"writeMode\"])\n",
    "        .save())    \n",
    "\n",
    "def writeDfToNeo (df, cypherQuery):\n",
    "    result = (df.write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", Neo4jConnectionInfo[\"URI\"])\n",
    "        .option(\"authentication.basic.username\", Neo4jConnectionInfo[\"Username\"])\n",
    "        .option(\"authentication.basic.password\", Neo4jConnectionInfo[\"Password\"])\n",
    "        .option(\"batch.size\", 25000)\n",
    "        .option(\"query\", cypherQuery)\n",
    "        .mode(LoadSettings[\"writeMode\"])\n",
    "        .save())\n",
    "    \n",
    "# tableSize = 10 means a 10x10 table\n",
    "def getPartitionsAndBatches (tableSize):\n",
    "    batches = []\n",
    "    for i in range(tableSize):\n",
    "        partitions = []\n",
    "        for j in range(tableSize):\n",
    "            k = (i+j)%tableSize\n",
    "            partitions.append(str(k) + '-' + str(j))\n",
    "        batches.append(partitions)\n",
    "    return batches\n",
    "\n",
    "batches = getPartitionsAndBatches(10)    \n",
    "\n",
    "def filter_by_partition_code(df, partitionColName, partitionSet):\n",
    "    return df.filter(col(partitionColName).isin(partitionSet))\n",
    "\n",
    "def writeRelTableToNeo(tableName, startNodeCol, endNodeCol, cypherQuery):\n",
    "    df = spark.read.table(tableName)\n",
    "    partitionColName = LoadSettings['partitionColName']\n",
    "    newDf = df.withColumn(partitionColName, concat_ws('-', substring(df[startNodeCol], -1, 1), substring(df[endNodeCol], -1, 1)))\n",
    "    # Create separate DataFrames for each batch\n",
    "    dataframes = {}\n",
    "    for index, partitionSet in enumerate(batches):\n",
    "        dataframes[index] = filter_by_partition_code(newDf, partitionColName, partitionSet)\n",
    "        parallelRelDf = dataframes[index].repartition(10, partitionColName)\n",
    "        print('Num partitions (' + str(index) + '): ' + str(parallelRelDf.rdd.getNumPartitions()))\n",
    "\n",
    "        writeDfToNeo(parallelRelDf, cypherQuery)\n",
    "\n",
    "def saveCsvToTable (csvName, tableName):\n",
    "    csvPath = LoadSettings[\"csvDir\"] + csvName\n",
    "    \n",
    "    df = (spark.read.format('csv')\n",
    "        .options(header='true', inferSchema='true')\n",
    "        .load(csvPath))\n",
    "    \n",
    "    df.write.saveAsTable(tableName)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe0669d-395a-4d80-86cc-77ac851b0837",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "saveCsvToTable(\"users-same-5.csv\", \"movie_users_same_5\")\n",
    "saveCsvToTable(\"users.csv\", \"movie_users\")\n",
    "saveCsvToTable(\"movies.csv\", \"movies\")\n",
    "saveCsvToTable(\"users-rated-same-movie-5.csv\", \"movie_user_to_user_rel\")\n",
    "saveCsvToTable(\"user-rated-movie-5.csv\", \"movie_user_to_movie_rel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b7811e-f6a6-4dcf-aa46-a93b2952e9dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "writeTableToNeo(\"movie_users\", \"MERGE (user:MovieUser {userId: event.userId})\")\n",
    "writeTableToNeo(\"movie_users_same_5\", \"MERGE (user:MovieUser {userId: event.userId})\")\n",
    "writeTableToNeo(\"movies\", \"MERGE (movie:Movie {movieImdbId: event.movieImdbId})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16343b33-fb38-48db-9307-5784d89e81da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# writeTableToNeoSinglePartition(\"movie_user_to_movie_rel\", \"\"\"\n",
    "#         MATCH (user:MovieUser {userId: event.userId})\n",
    "#         MATCH (movie:Movie {movieImdbId: event.movieImdbId})\n",
    "#         MERGE (user)-[:RATED_5]->(movie)\n",
    "#     \"\"\")\n",
    "\n",
    "writeTableToNeoSinglePartition(\"movie_user_to_user_rel\", \"\"\"\n",
    "        MATCH (user1:MovieUser {userId: event.userId1})\n",
    "        MATCH (user2:MovieUser {userId: event.userId2})\n",
    "        MERGE (user1)-[:GAVE_SAME_RATING]->(user2) \n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60597485-e2cd-4c91-bf0a-a1ba01e1a803",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-0', '1-1', '2-2', '3-3', '4-4', '5-5', '6-6', '7-7', '8-8', '9-9']\n",
      "Num partitions (0): 10\n",
      "['1-0', '2-1', '3-2', '4-3', '5-4', '6-5', '7-6', '8-7', '9-8', '0-9']\n",
      "Num partitions (1): 10\n",
      "['2-0', '3-1', '4-2', '5-3', '6-4', '7-5', '8-6', '9-7', '0-8', '1-9']\n",
      "Num partitions (2): 10\n",
      "['3-0', '4-1', '5-2', '6-3', '7-4', '8-5', '9-6', '0-7', '1-8', '2-9']\n",
      "Num partitions (3): 10\n",
      "['4-0', '5-1', '6-2', '7-3', '8-4', '9-5', '0-6', '1-7', '2-8', '3-9']\n",
      "Num partitions (4): 10\n",
      "['5-0', '6-1', '7-2', '8-3', '9-4', '0-5', '1-6', '2-7', '3-8', '4-9']\n",
      "Num partitions (5): 10\n",
      "['6-0', '7-1', '8-2', '9-3', '0-4', '1-5', '2-6', '3-7', '4-8', '5-9']\n",
      "Num partitions (6): 10\n",
      "['7-0', '8-1', '9-2', '0-3', '1-4', '2-5', '3-6', '4-7', '5-8', '6-9']\n",
      "Num partitions (7): 10\n",
      "['8-0', '9-1', '0-2', '1-3', '2-4', '3-5', '4-6', '5-7', '6-8', '7-9']\n",
      "Num partitions (8): 10\n",
      "['9-0', '0-1', '1-2', '2-3', '3-4', '4-5', '5-6', '6-7', '7-8', '8-9']\n",
      "Num partitions (9): 10\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import pmod, col, count, substring, concat_ws, udf\n",
    "\n",
    "# tableSize = 10 means a 10x10 table\n",
    "def getPartitionsAndBatches (tableSize):\n",
    "    batches = []\n",
    "    for i in range(tableSize):\n",
    "        partitions = []\n",
    "        for j in range(tableSize):\n",
    "            k = (i+j)%tableSize\n",
    "            partitions.append(str(k) + '-' + str(j))\n",
    "        batches.append(partitions)\n",
    "    return batches\n",
    "\n",
    "batches = getPartitionsAndBatches(10)\n",
    "\n",
    "get_partition_code_1 = udf(lambda x, y: str(math.floor((x or 0) / 1)) + '-' + str(math.floor((y or 0) / 1)), StringType())\n",
    "get_partition_code_10 = udf(lambda x, y: str(math.floor((x or 0) / 10)) + '-' + str(math.floor((y or 0) / 10)), StringType())\n",
    "get_partition_code_100 = udf(lambda x, y: str(math.floor((x or 0) / 100)) + '-' + str(math.floor((y or 0) / 100)), StringType())\n",
    "\n",
    "# Function to filter DataFrame by last digit of userId\n",
    "def filter_by_partition_code(df, colName, partitionSet):\n",
    "    # print(colName, partitionSet)\n",
    "    return df.filter(col(partitionColName).isin(partitionSet))\n",
    "\n",
    "partitionColName = 'partitionCode'\n",
    "df = spark.read.table('movie_user_to_movie_rel')\n",
    "\n",
    "# https://stackoverflow.com/questions/45512884/spark-dataframe-column-with-last-character-of-other-column\n",
    "\n",
    "newDf1 = df.withColumn(partitionColName, \\\n",
    "                      get_partition_code_1(substring(df.userId, -1, 1).cast(\"int\"), \\\n",
    "                                     substring(df.movieImdbId, -1, 1).cast(\"int\")) \\\n",
    "                      ) \n",
    "# newDf.show()\n",
    "\n",
    "newDf = newDf1\n",
    "\n",
    "def writeDfToNeo (df, cypherQuery):\n",
    "    result = (df.write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", Neo4jConnectionInfo[\"URI\"])\n",
    "        .option(\"authentication.basic.username\", Neo4jConnectionInfo[\"Username\"])\n",
    "        .option(\"authentication.basic.password\", Neo4jConnectionInfo[\"Password\"])\n",
    "        .option(\"batch.size\", 25000)\n",
    "        .option(\"query\", cypherQuery)\n",
    "        .mode(LoadSettings[\"writeMode\"])\n",
    "        .save())\n",
    "\n",
    "# Create separate DataFrames for each batch\n",
    "dataframes = {}\n",
    "for index, partitionSet in enumerate(batches):\n",
    "    print(partitionSet)\n",
    "    dataframes[index] = filter_by_partition_code(newDf, partitionColName, partitionSet)\n",
    "    parallelRelDf = dataframes[index].repartition(10, partitionColName)\n",
    "    print('Num partitions (' + str(index) + '): ' + str(parallelRelDf.rdd.getNumPartitions()))\n",
    "\n",
    "    writeDfToNeo(parallelRelDf, \"\"\"\n",
    "        MATCH (user:MovieUser {userId: event.userId})\n",
    "        MATCH (movie:Movie {movieImdbId: event.movieImdbId})\n",
    "        MERGE (user)-[:RATED_5]->(movie)\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5efc2a14-4b7b-416a-b9b1-6e6ba227afda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-0', '1-1', '2-2', '3-3', '4-4', '5-5', '6-6', '7-7', '8-8', '9-9']\n",
      "['1-0', '2-1', '3-2', '4-3', '5-4', '6-5', '7-6', '8-7', '9-8', '0-9']\n",
      "['2-0', '3-1', '4-2', '5-3', '6-4', '7-5', '8-6', '9-7', '0-8', '1-9']\n",
      "['3-0', '4-1', '5-2', '6-3', '7-4', '8-5', '9-6', '0-7', '1-8', '2-9']\n",
      "['4-0', '5-1', '6-2', '7-3', '8-4', '9-5', '0-6', '1-7', '2-8', '3-9']\n",
      "['5-0', '6-1', '7-2', '8-3', '9-4', '0-5', '1-6', '2-7', '3-8', '4-9']\n",
      "['6-0', '7-1', '8-2', '9-3', '0-4', '1-5', '2-6', '3-7', '4-8', '5-9']\n",
      "['7-0', '8-1', '9-2', '0-3', '1-4', '2-5', '3-6', '4-7', '5-8', '6-9']\n",
      "['8-0', '9-1', '0-2', '1-3', '2-4', '3-5', '4-6', '5-7', '6-8', '7-9']\n",
      "['9-0', '0-1', '1-2', '2-3', '3-4', '4-5', '5-6', '6-7', '7-8', '8-9']\n",
      "Num partitions (9): 10\n",
      "Processing parallel data frame 0\n",
      "Num partitions (9): 10\n",
      "Num partitions (9): 10\n",
      "Num partitions (9): 10\n",
      "Num partitions (9): 10\n",
      "Num partitions (9): 10\n",
      "Num partitions (9): 10\n",
      "Num partitions (9): 10\n",
      "Num partitions (9): 10\n",
      "Num partitions (9): 10\n",
      "Processing parallel data frame 1\n",
      "Processing parallel data frame 2\n",
      "Processing parallel data frame 3\n",
      "Processing parallel data frame 4\n",
      "Processing parallel data frame 5\n",
      "Processing parallel data frame 6\n",
      "Processing parallel data frame 7\n",
      "Processing parallel data frame 8\n",
      "Processing parallel data frame 9\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import pmod, col, count, substring, concat_ws, udf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Queue\n",
    "\n",
    "# tableSize = 10 means a 10x10 table\n",
    "def getPartitionsAndBatches (tableSize):\n",
    "    batches = []\n",
    "    for i in range(tableSize):\n",
    "        partitions = []\n",
    "        for j in range(tableSize):\n",
    "            k = (i+j)%tableSize\n",
    "            partitions.append(str(k) + '-' + str(j))\n",
    "        batches.append(partitions)\n",
    "    return batches\n",
    "\n",
    "batches = getPartitionsAndBatches(10)\n",
    "\n",
    "get_partition_code_1 = udf(lambda x, y: str(math.floor((x or 0) / 1)) + '-' + str(math.floor((y or 0) / 1)), StringType())\n",
    "get_partition_code_10 = udf(lambda x, y: str(math.floor((x or 0) / 10)) + '-' + str(math.floor((y or 0) / 10)), StringType())\n",
    "get_partition_code_100 = udf(lambda x, y: str(math.floor((x or 0) / 100)) + '-' + str(math.floor((y or 0) / 100)), StringType())\n",
    "\n",
    "# Function to filter DataFrame by last digit of userId\n",
    "def filter_by_partition_code(df, colName, partitionSet):\n",
    "    # print(colName, partitionSet)\n",
    "    return df.filter(col(partitionColName).isin(partitionSet))\n",
    "\n",
    "partitionColName = 'partitionCode'\n",
    "df = spark.read.table('movie_user_to_movie_rel')\n",
    "\n",
    "# https://stackoverflow.com/questions/45512884/spark-dataframe-column-with-last-character-of-other-column\n",
    "newDf1 = df.withColumn(partitionColName, \\\n",
    "                      get_partition_code_1(substring(df.userId, -1, 1).cast(\"int\"), \\\n",
    "                                     substring(df.movieImdbId, -1, 1).cast(\"int\")) \\\n",
    "                      ) \n",
    "\n",
    "# newDf.show()\n",
    "\n",
    "newDf = newDf1\n",
    "\n",
    "def filterDf(queue, newDf, partitionColName, partitionSet):\n",
    "    print(partitionSet)\n",
    "    partitionedDf = filter_by_partition_code(newDf, partitionColName, partitionSet)\n",
    "    parallelRelDf = partitionedDf.repartition(10, partitionColName)\n",
    "    print('Num partitions (' + str(index) + '): ' + str(parallelRelDf.rdd.getNumPartitions()))\n",
    "\n",
    "    # push data into the queue\n",
    "    queue.put(parallelRelDf)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # define the shared queue\n",
    "    queue = Queue()\n",
    "    # create the thread pool\n",
    "    n_tasks = 10\n",
    "    with ThreadPoolExecutor() as tpe:\n",
    "        # issue tasks to the thread pool\n",
    "        for index, partitionSet in enumerate(batches):\n",
    "            tpe.submit(filterDf, queue, newDf, partitionColName, partitionSet)\n",
    "        \n",
    "        # consume results from the queue\n",
    "        for i in range(n_tasks):\n",
    "            # get parallelRelDf from the queue\n",
    "            parallelRelDf = queue.get()\n",
    "\n",
    "            # row_count = parallelRelDf.count()\n",
    "\n",
    "            # print(f'The DataFrame has {row_count} rows.')\n",
    "            print(f'Processing parallel data frame {i}')\n",
    "\n",
    "            writeDfToNeo(parallelRelDf, \"\"\"\n",
    "                MATCH (user:MovieUser {userId: event.userId})\n",
    "                MATCH (movie:Movie {movieImdbId: event.movieImdbId})\n",
    "                MERGE (user)-[:RATED_5]->(movie)\n",
    "            \"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb3e4df-9ad6-4e56-9b98-e0de3d4e4c46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import pmod, col, count, substring, concat_ws, udf\n",
    "\n",
    "sameSameBatches = [[\"0-1\",\"1-0\",\"2-3\",\"3-2\",\"4-5\",\"5-4\",\"6-7\",\"7-6\",\"8-9\",\"9-8\"],[\"0-2\",\"2-0\",\"1-3\",\"3-1\",\"4-7\",\"7-4\",\"5-8\",\"8-5\",\"6-9\",\"9-6\"],[\"0-3\",\"3-0\",\"1-2\",\"2-1\",\"4-9\",\"9-4\",\"5-7\",\"7-5\",\"6-8\",\"8-6\"],[\"0-5\",\"5-0\",\"1-6\",\"6-1\",\"2-4\",\"4-2\",\"3-9\",\"9-3\",\"7-8\",\"8-7\"],[\"0-4\",\"4-0\",\"1-5\",\"5-1\",\"2-6\",\"6-2\",\"3-8\",\"8-3\",\"7-9\",\"9-7\"],[\"0-6\",\"6-0\",\"1-8\",\"8-1\",\"2-7\",\"7-2\",\"3-4\",\"4-3\",\"5-9\",\"9-5\"],[\"0-9\",\"9-0\",\"1-4\",\"4-1\",\"2-8\",\"8-2\",\"3-7\",\"7-3\",\"5-6\",\"6-5\"],[\"0-7\",\"7-0\",\"1-9\",\"9-1\",\"2-5\",\"5-2\",\"3-6\",\"6-3\",\"4-8\",\"8-4\"],[\"0-8\",\"8-0\",\"1-7\",\"7-1\",\"2-9\",\"9-2\",\"3-5\",\"5-3\",\"4-6\",\"6-4\"],[\"0-0\",\"1-1\",\"2-2\",\"3-3\",\"4-4\",\"5-5\",\"6-6\",\"7-7\",\"8-8\",\"9-9\"]]\n",
    "\n",
    "batches = sameSameBatches\n",
    "\n",
    "get_partition_code_1 = udf(lambda x, y: str(math.floor((x or 0) / 1)) + '-' + str(math.floor((y or 0) / 1)), StringType())\n",
    "get_partition_code_10 = udf(lambda x, y: str(math.floor((x or 0) / 10)) + '-' + str(math.floor((y or 0) / 10)), StringType())\n",
    "get_partition_code_100 = udf(lambda x, y: str(math.floor((x or 0) / 100)) + '-' + str(math.floor((y or 0) / 100)), StringType())\n",
    "\n",
    "# Function to filter DataFrame by last digit of userId\n",
    "def filter_by_partition_code(df, colName, partitionSet):\n",
    "    # print(colName, partitionSet)\n",
    "    return df.filter(col(partitionColName).isin(partitionSet))\n",
    "\n",
    "partitionColName = 'partitionCode'\n",
    "df = spark.read.table('movie_user_to_user_rel')\n",
    "\n",
    "newDf1 = df.withColumn(partitionColName, \\\n",
    "                      get_partition_code_1(substring(df.userId1, -1, 1).cast(\"int\"), \\\n",
    "                                     substring(df.userId2, -1, 1).cast(\"int\")) \\\n",
    "                      ) \n",
    "# newDf.show()\n",
    "newDf = newDf1\n",
    "\n",
    "# Create separate DataFrames for each batch\n",
    "dataframes = {}\n",
    "for index, partitionSet in enumerate(batches):\n",
    "    print(partitionSet)\n",
    "    dataframes[index] = filter_by_partition_code(newDf, partitionColName, partitionSet)\n",
    "    parallelRelDf = dataframes[index].repartition(10, partitionColName)\n",
    "    print('Num partitions (' + str(index) + '): ' + str(parallelRelDf.rdd.getNumPartitions()))\n",
    "\n",
    "    writeDfToNeo(parallelRelDf, \"\"\"\n",
    "        MATCH (user1:MovieUser {userId: event.userId1})\n",
    "        MATCH (user2:MovieUser {userId: event.userId2})\n",
    "        MERGE (user1)-[:GAVE_SAME_RATING]->(user2)\n",
    "    \"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Movies Rel Import",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
