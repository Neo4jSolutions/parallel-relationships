{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d064f73-f48a-4bc2-8aad-07f2bdd319fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Neo4jConnectionInfo = {\n",
    "    \"URI\": \"neo4j+s://62d77af8.databases.neo4j.io\", \n",
    "    \"Username\": dbutils.secrets.get(scope=\"neo4j\", key=\"username\"), \n",
    "    \"Password\": dbutils.secrets.get(scope=\"neo4j\", key=\"password\")\n",
    "}\n",
    "\n",
    "LoadSettings = {\n",
    "    \"csvDir\": \"file:/Workspace/Users/eric.monk@neo4j.com/mind_csvs/\",\n",
    "    \"writeMode\": \"Overwrite\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b886f29-dab1-4d29-80dd-36199eee8547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.defaultParallelism)\n",
    "# spark.conf.get(\"spark.sparkContext.defaultParallelism\")\n",
    "# print(spark.conf.get(\"spark.default.parallelism\"))\n",
    "#print(spark.conf.get(\"spark.executor.cores\"))\n",
    "print(spark.conf.get(\"spark.executor.extraClassPath\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65462a8c-c5f5-447c-8fa9-b0404f5105a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring, concat_ws\n",
    "\n",
    "df = spark.read.table('historic_clicks')\n",
    "\n",
    "# https://stackoverflow.com/questions/45512884/spark-dataframe-column-with-last-character-of-other-column\n",
    "# newDf = df.withColumn('partitionCode', substring(df.userId, -1, 1))\n",
    "newDf = df.withColumn('partitionCode', concat_ws('-', substring(df.userId, -1, 1), substring(df.newsId, -1, 1)))\n",
    "pandasDf = newDf.toPandas()\n",
    "grouped = pandasDf.groupby('partitionCode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8e0fc4-8ee7-478a-be21-0adbfb064875",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0', '0-0', '0-1', '0-2', '0-3', '0-4', '0-5', '0-6', '0-7', '0-8', '0-9', '1', '1-0', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-7', '1-8', '1-9', '2', '2-0', '2-1', '2-2', '2-3', '2-4', '2-5', '2-6', '2-7', '2-8', '2-9', '3', '3-0', '3-1', '3-2', '3-3', '3-4', '3-5', '3-6', '3-7', '3-8', '3-9', '4', '4-0', '4-1', '4-2', '4-3', '4-4', '4-5', '4-6', '4-7', '4-8', '4-9', '5', '5-0', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '5-7', '5-8', '5-9', '6', '6-0', '6-1', '6-2', '6-3', '6-4', '6-5', '6-6', '6-7', '6-8', '6-9', '7', '7-0', '7-1', '7-2', '7-3', '7-4', '7-5', '7-6', '7-7', '7-8', '7-9', '8', '8-0', '8-1', '8-2', '8-3', '8-4', '8-5', '8-6', '8-7', '8-8', '8-9', '9', '9-0', '9-1', '9-2', '9-3', '9-4', '9-5', '9-6', '9-7', '9-8', '9-9'])\n",
      "0: 220\n",
      "0-0: 21383\n",
      "0-1: 21066\n",
      "0-2: 19669\n",
      "0-3: 20359\n",
      "0-4: 20895\n",
      "0-5: 20377\n",
      "0-6: 22409\n",
      "0-7: 20413\n",
      "0-8: 21172\n",
      "0-9: 21869\n",
      "1: 211\n",
      "1-0: 20636\n",
      "1-1: 20120\n",
      "1-2: 18611\n",
      "1-3: 19713\n",
      "1-4: 20498\n",
      "1-5: 19936\n",
      "1-6: 22195\n",
      "1-7: 20076\n",
      "1-8: 20301\n",
      "1-9: 21227\n",
      "2: 220\n",
      "2-0: 20340\n",
      "2-1: 19963\n",
      "2-2: 18654\n",
      "2-3: 19556\n",
      "2-4: 19735\n",
      "2-5: 19604\n",
      "2-6: 21702\n",
      "2-7: 19741\n",
      "2-8: 20267\n",
      "2-9: 21058\n",
      "3: 201\n",
      "3-0: 20788\n",
      "3-1: 20356\n",
      "3-2: 18926\n",
      "3-3: 19919\n",
      "3-4: 20807\n",
      "3-5: 20167\n",
      "3-6: 22274\n",
      "3-7: 19939\n",
      "3-8: 20596\n",
      "3-9: 21051\n",
      "4: 206\n",
      "4-0: 20826\n",
      "4-1: 20633\n",
      "4-2: 18809\n",
      "4-3: 19922\n",
      "4-4: 20477\n",
      "4-5: 19874\n",
      "4-6: 22355\n",
      "4-7: 20097\n",
      "4-8: 20640\n",
      "4-9: 21395\n",
      "5: 222\n",
      "5-0: 20446\n",
      "5-1: 19978\n",
      "5-2: 18409\n",
      "5-3: 19485\n",
      "5-4: 19953\n",
      "5-5: 19682\n",
      "5-6: 21890\n",
      "5-7: 19484\n",
      "5-8: 20137\n",
      "5-9: 21020\n",
      "6: 203\n",
      "6-0: 20512\n",
      "6-1: 20575\n",
      "6-2: 19024\n",
      "6-3: 19945\n",
      "6-4: 20564\n",
      "6-5: 20343\n",
      "6-6: 21844\n",
      "6-7: 19996\n",
      "6-8: 20739\n",
      "6-9: 21731\n",
      "7: 215\n",
      "7-0: 20681\n",
      "7-1: 20262\n",
      "7-2: 18848\n",
      "7-3: 19990\n",
      "7-4: 20605\n",
      "7-5: 19994\n",
      "7-6: 21875\n",
      "7-7: 19987\n",
      "7-8: 20637\n",
      "7-9: 21517\n",
      "8: 215\n",
      "8-0: 20539\n",
      "8-1: 20489\n",
      "8-2: 18592\n",
      "8-3: 19808\n",
      "8-4: 20556\n",
      "8-5: 19994\n",
      "8-6: 22124\n",
      "8-7: 19913\n",
      "8-8: 20123\n",
      "8-9: 21209\n",
      "9: 209\n",
      "9-0: 20392\n",
      "9-1: 19976\n",
      "9-2: 18315\n",
      "9-3: 19491\n",
      "9-4: 20213\n",
      "9-5: 19719\n",
      "9-6: 21564\n",
      "9-7: 19811\n",
      "9-8: 20272\n",
      "9-9: 20981\n"
     ]
    }
   ],
   "source": [
    "# df.show()\n",
    "# df.columns\n",
    "# newDf.show()\n",
    "# print(grouped)\n",
    "# grouped.describe()\n",
    "print(grouped.groups.keys())\n",
    "sparkDfs = {}\n",
    "\n",
    "for key in grouped.groups.keys():\n",
    "    segment = grouped.get_group(key)\n",
    "    segment.describe()\n",
    "    print(key + ': ' + str(len(segment.index)))\n",
    "\n",
    "# for key in grouped.groups.keys():\n",
    "#     sparkDfs[key] = spark.createDataFrame(grouped.get_group(key))\n",
    "\n",
    "# print(sparkDfs)\n",
    "#df00 = grouped.get_group('0-0')\n",
    "#print(df00)\n",
    "# df00.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbff16b8-26e4-447c-a535-a42518474e52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for userIds with last digit 1:\n",
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|   u11|\n",
      "|   u21|\n",
      "|   u31|\n",
      "|   u41|\n",
      "|   u51|\n",
      "+------+\n",
      "\n",
      "DataFrame for userIds with last digit 2:\n",
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|   u12|\n",
      "|   u22|\n",
      "|   u32|\n",
      "+------+\n",
      "\n",
      "DataFrame for userIds with last digit 3:\n",
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|   u13|\n",
      "|   u23|\n",
      "|   u53|\n",
      "|   u43|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample data\n",
    "data = [(\"u11\",), (\"u21\",), (\"u31\",), (\"u41\",), (\"u51\",),\n",
    "        (\"u12\",), (\"u13\",), (\"u22\",), (\"u23\",), (\"u53\",), (\"u43\",), (\"u32\",)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"userId\"])\n",
    "\n",
    "# Function to filter DataFrame by last digit of userId\n",
    "def filter_by_last_digit(df, digit):\n",
    "    return df.filter(col(\"userId\").substr(-1, 1) == digit)\n",
    "\n",
    "# Create separate DataFrames for each set of userIds with the same last digit\n",
    "distinct_digits = df.select(df[\"userId\"].substr(-1, 1)).distinct().collect()\n",
    "dataframes = {}\n",
    "for digit_row in distinct_digits:\n",
    "    digit = digit_row[0]\n",
    "    dataframes[digit] = filter_by_last_digit(df, digit)\n",
    "\n",
    "# Print and show the DataFrames\n",
    "for digit, dataframe in dataframes.items():\n",
    "    print(f\"DataFrame for userIds with last digit {digit}:\")\n",
    "    dataframe.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f54d0c64-b80c-4a4e-ab2b-c06ca6774c77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions (0): 10\n",
      "0-0: 93906\n",
      "1-1: 79137\n",
      "2-2: 92085\n",
      "3-3: 69603\n",
      "4-4: 76708\n",
      "5-5: 75705\n",
      "6-6: 83277\n",
      "7-7: 89520\n",
      "8-8: 79672\n",
      "9-9: 84095\n",
      "Num partitions (1): 10\n",
      "0-9: 85705\n",
      "1-0: 94758\n",
      "2-1: 78615\n",
      "3-2: 91884\n",
      "4-3: 70615\n",
      "5-4: 73935\n",
      "6-5: 76983\n",
      "7-6: 85377\n",
      "8-7: 84643\n",
      "9-8: 81109\n",
      "Num partitions (2): 10\n",
      "0-8: 82172\n",
      "1-9: 86080\n",
      "2-0: 93423\n",
      "3-1: 78441\n",
      "4-2: 93274\n",
      "5-3: 68244\n",
      "6-4: 75390\n",
      "7-5: 78887\n",
      "8-6: 81025\n",
      "9-7: 85658\n",
      "Num partitions (3): 10\n",
      "0-7: 87132\n",
      "1-8: 82562\n",
      "2-9: 85319\n",
      "3-0: 93452\n",
      "4-1: 80311\n",
      "5-2: 90928\n",
      "6-3: 70014\n",
      "7-4: 77653\n",
      "8-5: 74449\n",
      "9-6: 82441\n",
      "Num partitions (4): 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, substring, concat_ws\n",
    "\n",
    "# tableSize = 10 means a 10x10 table\n",
    "def getPartitionsAndBatches (tableSize):\n",
    "    batches = []\n",
    "    for i in range(tableSize):\n",
    "        partitions = []\n",
    "        for j in range(tableSize):\n",
    "            k = (i+j)%tableSize\n",
    "            partitions.append(str(k) + '-' + str(j))\n",
    "        batches.append(partitions)\n",
    "    return batches\n",
    "\n",
    "batches = getPartitionsAndBatches(10)\n",
    "# batches = getPartitionsAndBatches(100)\n",
    "\n",
    "# Function to filter DataFrame by last digit of userId\n",
    "def filter_by_partition_code(df, colName, partitionSet):\n",
    "    # print(colName, partitionSet)\n",
    "    return df.filter(col(partitionColName).isin(partitionSet))\n",
    "\n",
    "partitionColName = 'partitionCode'\n",
    "# df = spark.read.table('historic_clicks')\n",
    "df = spark.read.table('did_not_clicks')\n",
    "\n",
    "# https://stackoverflow.com/questions/45512884/spark-dataframe-column-with-last-character-of-other-column\n",
    "# newDf = df.withColumn('partitionCode', substring(df.userId, -1, 1))\n",
    "newDf = df.withColumn(partitionColName, concat_ws('-', substring(df.userId, -1, 1), substring(df.newsId, -1, 1)))\n",
    "# newDf = df.withColumn(partitionColName, concat_ws('-', substring(df.userId, -2, 2), substring(df.newsId, -2, 2)))\n",
    "# newDf.show()\n",
    "\n",
    "def writeDfToNeo (df, cypherQuery):\n",
    "    result = (df.write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", Neo4jConnectionInfo[\"URI\"])\n",
    "        .option(\"authentication.basic.username\", Neo4jConnectionInfo[\"Username\"])\n",
    "        .option(\"authentication.basic.password\", Neo4jConnectionInfo[\"Password\"])\n",
    "        .option(\"batch.size\", 25000)\n",
    "        .option(\"query\", cypherQuery)\n",
    "        .mode(LoadSettings[\"writeMode\"])\n",
    "        .save())\n",
    "\n",
    "\n",
    "# Create separate DataFrames for each batch\n",
    "dataframes = {}\n",
    "for index, partitionSet in enumerate(batches):\n",
    "    # print(partitionSet)\n",
    "    dataframes[index] = filter_by_partition_code(newDf, partitionColName, partitionSet)\n",
    "    # parallelRelDf = dataframes[index].repartition(partitionColName)\n",
    "    parallelRelDf = dataframes[index].repartition(10, partitionColName)\n",
    "    # parallelRelDf = dataframes[index].repartition(100, partitionColName)\n",
    "    print('Num partitions (' + str(index) + '): ' + str(parallelRelDf.rdd.getNumPartitions()))\n",
    "\n",
    "    pandasDf = parallelRelDf.toPandas()\n",
    "    grouped = pandasDf.groupby(partitionColName)\n",
    "    # grouped.describe()\n",
    "    for key in grouped.groups.keys():\n",
    "        segment = grouped.get_group(key)\n",
    "        segment.describe()\n",
    "        print(key + ': ' + str(len(segment.index)))    \n",
    "\n",
    "    # parallelRelDf.show()\n",
    "    # writeDfToNeo(parallelRelDf, \"\"\"\n",
    "    #         MATCH(user:User {userId: event.userId})\n",
    "    #         MATCH(news:News {newsId: event.newsId})\n",
    "    #         MERGE(user)-[r:HISTORICALLY_CLICKED {splitSet: event.splitSet}]->(news)\n",
    "    #         RETURN count(r)\n",
    "    # \"\"\")\n",
    "\n",
    "    # writeDfToNeo(parallelRelDf, \"\"\"\n",
    "    #     MATCH(user:User {userId: event.userId})\n",
    "    #     MATCH(news:News {newsId: event.newsId})\n",
    "    #     MERGE(user)-[:DID_NOT_CLICK]->(news)\n",
    "    # \"\"\")\n",
    "\n",
    "        # bad - generates the wrong number of rels\n",
    "        # WHERE NOT EXISTS{(user)-[:DID_NOT_CLICK]->(news)}\n",
    "        # CREATE(user)-[r:DID_NOT_CLICK]->(news)\n",
    "\n",
    "        # MERGE(user)-[r:DID_NOT_CLICK {\n",
    "        #     splitSet: event.splitSet,\n",
    "        #     impressionId: event.impressionId,\n",
    "        #     impressionTime: event.time\n",
    "        # }]->(news)\n",
    "\n",
    "\n",
    "# Print and show the DataFrames\n",
    "# for index, dataframe in dataframes.items():\n",
    "#     print(f\"DataFrame for index {index}:\")\n",
    "#     dataframe.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db81fa05-f0eb-4ac2-81c3-a2608eae64a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, substring, concat_ws\n",
    "\n",
    "# tableSize = 10 means a 10x10 table\n",
    "# df = spark.read.table('historic_clicks').repartition(1)\n",
    "df = spark.read.table('did_not_clicks').repartition(1)\n",
    "\n",
    "def writeDfToNeo (df, cypherQuery):\n",
    "    result = (df.write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", Neo4jConnectionInfo[\"URI\"])\n",
    "        .option(\"authentication.basic.username\", Neo4jConnectionInfo[\"Username\"])\n",
    "        .option(\"authentication.basic.password\", Neo4jConnectionInfo[\"Password\"])\n",
    "        .option(\"query\", cypherQuery)\n",
    "        .option(\"batch.size\", 25000)\n",
    "        .mode(LoadSettings[\"writeMode\"])\n",
    "        .save())\n",
    "\n",
    "\n",
    "# writeDfToNeo(df, \"\"\"\n",
    "#         MATCH(user:User {userId: event.userId})\n",
    "#         MATCH(news:News {newsId: event.newsId})\n",
    "#         MERGE(user)-[r:HISTORICALLY_CLICKED {splitSet: event.splitSet}]->(news)\n",
    "#         RETURN count(r)\n",
    "# \"\"\")\n",
    "\n",
    "writeDfToNeo(df, \"\"\"\n",
    "    MATCH(user:User {userId: event.userId})\n",
    "    MATCH(news:News {newsId: event.newsId})\n",
    "    MERGE(user)-[r:DID_NOT_CLICK {\n",
    "        splitSet: event.splitSet,\n",
    "        impressionId: event.impressionId,\n",
    "        impressionTime: event.time\n",
    "    }]->(news)\n",
    "\"\"\")\n",
    "\n",
    "    # MERGE(user)-[:DID_NOT_CLICK]->(news)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80411089-0da4-49bf-9e6d-d9c66cc53af9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------------------+--------+------+-------------+-------+\n",
      "|impressionId|userId|               time|splitSet|newsId|partitionCode|sortVal|\n",
      "+------------+------+-------------------+--------+------+-------------+-------+\n",
      "|      120614|U18360|2019-11-12 07:20:32|   TRAIN| N3737|            7|   7836|\n",
      "|      120614|U18360|2019-11-12 07:20:32|   TRAIN|N28327|            7|   7836|\n",
      "|      120614|U18360|2019-11-12 07:20:32|   TRAIN|N44397|            7|   7836|\n",
      "|      120614|U18360|2019-11-12 07:20:32|   TRAIN|N34657|            7|   7836|\n",
      "|      120615|U54668|2019-11-09 08:17:07|   TRAIN|N43377|            7|   5466|\n",
      "|      120615|U54668|2019-11-09 08:17:07|   TRAIN|N54287|            7|   5466|\n",
      "|      120615|U54668|2019-11-09 08:17:07|   TRAIN|N12977|            7|   5466|\n",
      "|      120615|U54668|2019-11-09 08:17:07|   TRAIN|N53617|            7|   5466|\n",
      "|      120615|U54668|2019-11-09 08:17:07|   TRAIN|N37377|            7|   5466|\n",
      "|      120615|U54668|2019-11-09 08:17:07|   TRAIN| N4247|            7|   5466|\n",
      "|      120615|U54668|2019-11-09 08:17:07|   TRAIN|N25467|            7|   5466|\n",
      "|      120616|U71245|2019-11-09 15:49:05|   TRAIN|N53017|            7|   2124|\n",
      "|      120616|U71245|2019-11-09 15:49:05|   TRAIN|N61407|            7|   2124|\n",
      "|      120616|U71245|2019-11-09 15:49:05|   TRAIN|N35567|            7|   2124|\n",
      "|      120617|U16497|2019-11-14 11:20:44|   TRAIN|N41387|            7|   4649|\n",
      "|      120617|U16497|2019-11-14 11:20:44|   TRAIN| N6837|            7|   4649|\n",
      "|      120617|U16497|2019-11-14 11:20:44|   TRAIN| N6477|            7|   4649|\n",
      "|      120618|U22017|2019-11-13 06:24:11|   TRAIN|N13907|            7|   4201|\n",
      "|      120618|U22017|2019-11-13 06:24:11|   TRAIN|N42977|            7|   4201|\n",
      "|      120618|U22017|2019-11-13 06:24:11|   TRAIN|N35047|            7|   4201|\n",
      "+------------+------+-------------------+--------+------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-395196616532658>, line 27\u001b[0m\n",
       "\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# .orderBy(sortColName, 'userId'))\u001b[39;00m\n",
       "\u001b[1;32m     25\u001b[0m sortedPartitionedDf\u001b[38;5;241m.\u001b[39mshow()\n",
       "\u001b[0;32m---> 27\u001b[0m writeDfToNeo(sortedPartitionedDf, \u001b[38;5;124m\"\"\"\u001b[39m\n",
       "\u001b[1;32m     28\u001b[0m \u001b[38;5;124m        MATCH(user:User \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124muserId: event.userId})\u001b[39m\n",
       "\u001b[1;32m     29\u001b[0m \u001b[38;5;124m        MATCH(news:News \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mnewsId: event.newsId})\u001b[39m\n",
       "\u001b[1;32m     30\u001b[0m \u001b[38;5;124m        MERGE(user)-[:DID_NOT_CLICK]->(news)\u001b[39m\n",
       "\u001b[1;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
       "\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# MERGE(user)-[r:HISTORICALLY_CLICKED {splitSet: event.splitSet}]->(news)\u001b[39;00m\n",
       "\n",
       "File \u001b[0;32m<command-3170225426673041>, line 41\u001b[0m, in \u001b[0;36mwriteDfToNeo\u001b[0;34m(df, cypherQuery)\u001b[0m\n",
       "\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriteDfToNeo\u001b[39m (df, cypherQuery):\n",
       "\u001b[1;32m     33\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n",
       "\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.neo4j.spark.DataSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNeo4jConnectionInfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mURI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthentication.basic.username\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNeo4jConnectionInfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUsername\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthentication.basic.password\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNeo4jConnectionInfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch.size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25000\u001b[39;49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcypherQuery\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLoadSettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwriteMode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[0;32m---> 41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n",
       "\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n",
       "\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    190\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8132.save.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 89.0 failed 4 times, most recent failure: Lost task 8.3 in stage 89.0 (TID 579) (10.177.205.63 executor 2): org.neo4j.driver.exceptions.TransientException: ForsetiClient[transactionId=1487, clientId=8] can't acquire ExclusiveLock{owner=ForsetiClient[transactionId=1483, clientId=1]} on NODE_RELATIONSHIP_GROUP_DELETE(18138) because holders of that lock are waiting for ForsetiClient[transactionId=1487, clientId=8].\n",
       " Wait list:ExclusiveLock[\n",
       "Client[1483] waits for [ForsetiClient[transactionId=1487, clientId=8]]]\n",
       "\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)\n",
       "\tat org.neo4j.driver.internal.InternalTransaction.run(InternalTransaction.java:58)\n",
       "\tat org.neo4j.driver.internal.AbstractQueryRunner.run(AbstractQueryRunner.java:34)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:67)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:48)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:22)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:559)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:505)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1743)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:552)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:482)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:557)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:445)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:196)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:899)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1709)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:902)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:797)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n",
       "\t\tat org.neo4j.driver.internal.util.ErrorUtil.newNeo4jError(ErrorUtil.java:80)\n",
       "\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageDispatcher.handleFailureMessage(InboundMessageDispatcher.java:107)\n",
       "\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.unpackFailureMessage(CommonMessageReader.java:75)\n",
       "\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.read(CommonMessageReader.java:53)\n",
       "\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:81)\n",
       "\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:37)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\n",
       "\t\tat org.neo4j.driver.internal.async.inbound.MessageDecoder.channelRead(MessageDecoder.java:42)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:333)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:454)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1475)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1338)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1387)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
       "\t\t... 1 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3628)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3559)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3546)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3546)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1521)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1521)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1521)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3875)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3787)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3775)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1245)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1233)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2959)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2942)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:442)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:416)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:295)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:394)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:393)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:295)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:286)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:286)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:283)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:511)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:210)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1138)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:153)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:460)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:285)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:259)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:280)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:265)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:265)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:265)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:217)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:214)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:350)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:956)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:347)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n",
       "\tat sun.reflect.GeneratedMethodAccessor432.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.neo4j.driver.exceptions.TransientException: ForsetiClient[transactionId=1487, clientId=8] can't acquire ExclusiveLock{owner=ForsetiClient[transactionId=1483, clientId=1]} on NODE_RELATIONSHIP_GROUP_DELETE(18138) because holders of that lock are waiting for ForsetiClient[transactionId=1487, clientId=8].\n",
       " Wait list:ExclusiveLock[\n",
       "Client[1483] waits for [ForsetiClient[transactionId=1487, clientId=8]]]\n",
       "\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)\n",
       "\tat org.neo4j.driver.internal.InternalTransaction.run(InternalTransaction.java:58)\n",
       "\tat org.neo4j.driver.internal.AbstractQueryRunner.run(AbstractQueryRunner.java:34)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:67)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:48)\n",
       "\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:22)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:559)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:505)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1743)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:552)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:482)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:557)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:445)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:196)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:899)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1709)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:902)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:797)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n",
       "\t\tat org.neo4j.driver.internal.util.ErrorUtil.newNeo4jError(ErrorUtil.java:80)\n",
       "\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageDispatcher.handleFailureMessage(InboundMessageDispatcher.java:107)\n",
       "\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.unpackFailureMessage(CommonMessageReader.java:75)\n",
       "\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.read(CommonMessageReader.java:53)\n",
       "\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:81)\n",
       "\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:37)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\n",
       "\t\tat org.neo4j.driver.internal.async.inbound.MessageDecoder.channelRead(MessageDecoder.java:42)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:333)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:454)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1475)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1338)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1387)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
       "\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
       "\t\t... 1 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-395196616532658>, line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# .orderBy(sortColName, 'userId'))\u001b[39;00m\n\u001b[1;32m     25\u001b[0m sortedPartitionedDf\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 27\u001b[0m writeDfToNeo(sortedPartitionedDf, \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m        MATCH(user:User \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124muserId: event.userId})\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m        MATCH(news:News \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mnewsId: event.newsId})\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m        MERGE(user)-[:DID_NOT_CLICK]->(news)\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# MERGE(user)-[r:HISTORICALLY_CLICKED {splitSet: event.splitSet}]->(news)\u001b[39;00m\n\nFile \u001b[0;32m<command-3170225426673041>, line 41\u001b[0m, in \u001b[0;36mwriteDfToNeo\u001b[0;34m(df, cypherQuery)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriteDfToNeo\u001b[39m (df, cypherQuery):\n\u001b[1;32m     33\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.neo4j.spark.DataSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNeo4jConnectionInfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mURI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthentication.basic.username\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNeo4jConnectionInfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUsername\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthentication.basic.password\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNeo4jConnectionInfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch.size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcypherQuery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLoadSettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwriteMode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8132.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 89.0 failed 4 times, most recent failure: Lost task 8.3 in stage 89.0 (TID 579) (10.177.205.63 executor 2): org.neo4j.driver.exceptions.TransientException: ForsetiClient[transactionId=1487, clientId=8] can't acquire ExclusiveLock{owner=ForsetiClient[transactionId=1483, clientId=1]} on NODE_RELATIONSHIP_GROUP_DELETE(18138) because holders of that lock are waiting for ForsetiClient[transactionId=1487, clientId=8].\n Wait list:ExclusiveLock[\nClient[1483] waits for [ForsetiClient[transactionId=1487, clientId=8]]]\n\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)\n\tat org.neo4j.driver.internal.InternalTransaction.run(InternalTransaction.java:58)\n\tat org.neo4j.driver.internal.AbstractQueryRunner.run(AbstractQueryRunner.java:34)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:67)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:48)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:22)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:559)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1743)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:552)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:482)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:557)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:445)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:196)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:899)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1709)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:902)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:797)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n\t\tat org.neo4j.driver.internal.util.ErrorUtil.newNeo4jError(ErrorUtil.java:80)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageDispatcher.handleFailureMessage(InboundMessageDispatcher.java:107)\n\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.unpackFailureMessage(CommonMessageReader.java:75)\n\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.read(CommonMessageReader.java:53)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:81)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:37)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\n\t\tat org.neo4j.driver.internal.async.inbound.MessageDecoder.channelRead(MessageDecoder.java:42)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:333)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:454)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1475)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1338)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1387)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3628)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3559)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3546)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3546)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1521)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1521)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1521)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3875)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3787)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3775)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1245)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1233)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2942)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:442)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:416)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:295)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:394)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:393)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:295)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:286)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:286)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:283)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:511)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:210)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1138)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:153)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:460)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:285)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:259)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:265)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:265)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:217)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:350)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:956)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:347)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n\tat sun.reflect.GeneratedMethodAccessor432.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.neo4j.driver.exceptions.TransientException: ForsetiClient[transactionId=1487, clientId=8] can't acquire ExclusiveLock{owner=ForsetiClient[transactionId=1483, clientId=1]} on NODE_RELATIONSHIP_GROUP_DELETE(18138) because holders of that lock are waiting for ForsetiClient[transactionId=1487, clientId=8].\n Wait list:ExclusiveLock[\nClient[1483] waits for [ForsetiClient[transactionId=1487, clientId=8]]]\n\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)\n\tat org.neo4j.driver.internal.InternalTransaction.run(InternalTransaction.java:58)\n\tat org.neo4j.driver.internal.AbstractQueryRunner.run(AbstractQueryRunner.java:34)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:67)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n\tat org.neo4j.spark.writer.BaseDataWriter.writeBatch(BaseDataWriter.scala:99)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:48)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:22)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:559)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1743)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:552)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:482)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:557)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:445)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:196)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:899)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1709)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:902)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:797)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n\t\tat org.neo4j.driver.internal.util.ErrorUtil.newNeo4jError(ErrorUtil.java:80)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageDispatcher.handleFailureMessage(InboundMessageDispatcher.java:107)\n\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.unpackFailureMessage(CommonMessageReader.java:75)\n\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.read(CommonMessageReader.java:53)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:81)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:37)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\n\t\tat org.neo4j.driver.internal.async.inbound.MessageDecoder.channelRead(MessageDecoder.java:42)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:333)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:454)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1475)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1338)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1387)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 89.0 failed 4 times, most recent failure: Lost task 8.3 in stage 89.0 (TID 579) (10.177.205.63 executor 2): org.neo4j.driver.exceptions.TransientException: ForsetiClient[transactionId=1487, clientId=8] can't acquire ExclusiveLock{owner=ForsetiClient[transactionId=1483, clientId=1]} on NODE_RELATIONSHIP_GROUP_DELETE(18138) because holders of that lock are waiting for ForsetiClient[transactionId=1487, clientId=8].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import pmod, col, substring, concat_ws\n",
    "\n",
    "\n",
    "# df = spark.read.table('historic_clicks')\n",
    "df = spark.read.table('did_not_clicks')\n",
    "df = df.filter(df.newsId.isNotNull())\n",
    "\n",
    "# https://stackoverflow.com/questions/45512884/spark-dataframe-column-with-last-character-of-other-column\n",
    "partitionColName = 'partitionCode'\n",
    "sortColName = 'sortVal'\n",
    "\n",
    "sortedPartitionedDf = (df\n",
    "    .withColumn(partitionColName, substring(df.newsId, -1, 1))    \n",
    "    # .withColumn(sortColName, substring(df.userId, -4, 4))\n",
    "    .withColumn(sortColName, concat_ws('',  \\\n",
    "                                    pmod(substring(df.userId, -1, 1).cast(\"int\") +  \\\n",
    "                                    substring(df.newsId, -1, 1).cast(\"int\"), 10).cast(\"string\"), \\\n",
    "                                    substring(df.userId, -4, 3)))\n",
    "\n",
    "    .orderBy(sortColName)\n",
    "    .repartition(10, partitionColName))\n",
    "    # .orderBy(sortColName, 'userId'))\n",
    "\n",
    "sortedPartitionedDf.show()\n",
    "\n",
    "writeDfToNeo(sortedPartitionedDf, \"\"\"\n",
    "        MATCH(user:User {userId: event.userId})\n",
    "        MATCH(news:News {newsId: event.newsId})\n",
    "        MERGE(user)-[:DID_NOT_CLICK]->(news)\n",
    "\"\"\")\n",
    "\n",
    "# MERGE(user)-[r:HISTORICALLY_CLICKED {splitSet: event.splitSet}]->(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5b419c-32b7-4652-bdc4-f7c6f085bdc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0_0', '1_1', '2_2', '3_3'], ['1_0', '2_1', '3_2', '0_3'], ['2_0', '3_1', '0_2', '1_3']]\n"
     ]
    }
   ],
   "source": [
    "myitems = []\n",
    "for i in range(4):\n",
    "    myset = []\n",
    "    for j in range(4):\n",
    "        k = (i+j)%4\n",
    "        myset.append(str(k) + '_' + str(j))\n",
    "    myitems.append(myset)\n",
    "\n",
    "print(myitems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca8ddab-fe36-41da-bafe-97a3c7e6a3b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-0', '1-1', '2-2', '3-3', '4-4', '5-5', '6-6', '7-7', '8-8', '9-9']\n",
      "Num partitions (0): 10\n",
      "['1-0', '2-1', '3-2', '4-3', '5-4', '6-5', '7-6', '8-7', '9-8', '0-9']\n",
      "Num partitions (1): 10\n",
      "['2-0', '3-1', '4-2', '5-3', '6-4', '7-5', '8-6', '9-7', '0-8', '1-9']\n",
      "Num partitions (2): 10\n",
      "['3-0', '4-1', '5-2', '6-3', '7-4', '8-5', '9-6', '0-7', '1-8', '2-9']\n",
      "Num partitions (3): 10\n",
      "['4-0', '5-1', '6-2', '7-3', '8-4', '9-5', '0-6', '1-7', '2-8', '3-9']\n",
      "Num partitions (4): 10\n",
      "['5-0', '6-1', '7-2', '8-3', '9-4', '0-5', '1-6', '2-7', '3-8', '4-9']\n",
      "Num partitions (5): 10\n",
      "['6-0', '7-1', '8-2', '9-3', '0-4', '1-5', '2-6', '3-7', '4-8', '5-9']\n",
      "Num partitions (6): 10\n",
      "['7-0', '8-1', '9-2', '0-3', '1-4', '2-5', '3-6', '4-7', '5-8', '6-9']\n",
      "Num partitions (7): 10\n",
      "['8-0', '9-1', '0-2', '1-3', '2-4', '3-5', '4-6', '5-7', '6-8', '7-9']\n",
      "Num partitions (8): 10\n",
      "['9-0', '0-1', '1-2', '2-3', '3-4', '4-5', '5-6', '6-7', '7-8', '8-9']\n",
      "Num partitions (9): 10\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import pmod, col, count, substring, concat_ws, udf\n",
    "\n",
    "# tableSize = 10 means a 10x10 table\n",
    "def getPartitionsAndBatches (tableSize):\n",
    "    batches = []\n",
    "    for i in range(tableSize):\n",
    "        partitions = []\n",
    "        for j in range(tableSize):\n",
    "            k = (i+j)%tableSize\n",
    "            partitions.append(str(k) + '-' + str(j))\n",
    "        batches.append(partitions)\n",
    "    return batches\n",
    "\n",
    "# batches = getPartitionsAndBatches(8)\n",
    "batches = getPartitionsAndBatches(10)\n",
    "# batches = getPartitionsAndBatches(100)\n",
    "\n",
    "increment_udf = udf(lambda x: x + 1, IntegerType())\n",
    "increment_udf2 = udf(lambda x, y: x + y, IntegerType())\n",
    "# get_partition_code = udf(lambda x, y, z: str(math.floor((x or 0) / z)) + '-' + str(math.floor((y or 0) / z)), StringType())\n",
    "get_partition_code_1 = udf(lambda x, y: str(math.floor((x or 0) / 1)) + '-' + str(math.floor((y or 0) / 1)), StringType())\n",
    "get_partition_code_10 = udf(lambda x, y: str(math.floor((x or 0) / 10)) + '-' + str(math.floor((y or 0) / 10)), StringType())\n",
    "get_partition_code_100 = udf(lambda x, y: str(math.floor((x or 0) / 100)) + '-' + str(math.floor((y or 0) / 100)), StringType())\n",
    "# get_partition_code = udf(lambda x, y: math.floor(x / 25), IntegerType())\n",
    "\n",
    "# Function to filter DataFrame by last digit of userId\n",
    "def filter_by_partition_code(df, colName, partitionSet):\n",
    "    # print(colName, partitionSet)\n",
    "    return df.filter(col(partitionColName).isin(partitionSet))\n",
    "\n",
    "partitionColName = 'partitionCode'\n",
    "partitionColName2 = 'partitionCode2'\n",
    "df = spark.read.table('historic_clicks')\n",
    "# df = spark.read.table('did_not_clicks')\n",
    "\n",
    "# https://stackoverflow.com/questions/45512884/spark-dataframe-column-with-last-character-of-other-column\n",
    "# newDf = df.withColumn('partitionCode', substring(df.userId, -1, 1))\n",
    "# newDf = df.withColumn(partitionColName, concat_ws('-', substring(df.userId, -1, 1), substring(df.newsId, -1, 1)))\n",
    "# newDf = df.withColumn(partitionColName, concat_ws('-', substring(df.userId, -2, 2), substring(df.newsId, -2, 2)))\n",
    "# newDf = df.withColumn(partitionColName, \\\n",
    "#                       get_partition_code(substring(df.userId, -2, 2).cast(\"int\"), \\\n",
    "#                                      substring(df.newsId, -2, 2).cast(\"int\")) \\\n",
    "#                       )\n",
    "\n",
    "newDf1 = df.withColumn(partitionColName, \\\n",
    "                      get_partition_code_1(substring(df.userId, -1, 1).cast(\"int\"), \\\n",
    "                                     substring(df.newsId, -1, 1).cast(\"int\")) \\\n",
    "                      ) \n",
    "\n",
    "# newDf2 = df.withColumn(partitionColName, \\\n",
    "#                       get_partition_code_10(substring(df.userId, -2, 2).cast(\"int\"), \\\n",
    "#                                      substring(df.newsId, -2, 2).cast(\"int\")) \\\n",
    "#                       )                       \n",
    "\n",
    "# newDf3 = df.withColumn(partitionColName, \\\n",
    "#                       get_partition_code_100(substring(df.userId, -3, 3).cast(\"int\"), \\\n",
    "#                                      substring(df.newsId, -3, 3).cast(\"int\")) \\\n",
    "#                       ) \n",
    "\n",
    "# newDf.show()\n",
    "\n",
    "newDf = newDf1\n",
    "\n",
    "# print()\n",
    "# print('Result 1')\n",
    "# countResult = newDf1.groupBy(partitionColName).count()\n",
    "# pandasResult = countResult.toPandas()\n",
    "# print(pandasResult.to_string())\n",
    "\n",
    "# print()\n",
    "# print('Result 10')\n",
    "# countResult = newDf2.groupBy(partitionColName).count()\n",
    "# pandasResult = countResult.toPandas()\n",
    "# print(pandasResult.to_string())\n",
    "\n",
    "# print()\n",
    "# print('Result 100')\n",
    "# countResult = newDf3.groupBy(partitionColName).count()\n",
    "# pandasResult = countResult.toPandas()\n",
    "# print(pandasResult.to_string())\n",
    "\n",
    "\n",
    "def writeDfToNeo (df, cypherQuery):\n",
    "    result = (df.write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", Neo4jConnectionInfo[\"URI\"])\n",
    "        .option(\"authentication.basic.username\", Neo4jConnectionInfo[\"Username\"])\n",
    "        .option(\"authentication.basic.password\", Neo4jConnectionInfo[\"Password\"])\n",
    "        .option(\"batch.size\", 25000)\n",
    "        .option(\"query\", cypherQuery)\n",
    "        .mode(LoadSettings[\"writeMode\"])\n",
    "        .save())\n",
    "\n",
    "\n",
    "# Create separate DataFrames for each batch\n",
    "dataframes = {}\n",
    "for index, partitionSet in enumerate(batches):\n",
    "    print(partitionSet)\n",
    "    dataframes[index] = filter_by_partition_code(newDf, partitionColName, partitionSet)\n",
    "    # parallelRelDf = dataframes[index].repartition(partitionColName)\n",
    "    # parallelRelDf = dataframes[index].repartition(5, partitionColName)\n",
    "    # parallelRelDf = dataframes[index].repartition(8, partitionColName)\n",
    "    parallelRelDf = dataframes[index].repartition(10, partitionColName)\n",
    "    # parallelRelDf = dataframes[index].repartition(20, partitionColName)\n",
    "    # parallelRelDf = dataframes[index].repartition(100, partitionColName)\n",
    "    print('Num partitions (' + str(index) + '): ' + str(parallelRelDf.rdd.getNumPartitions()))\n",
    "\n",
    "    # pandasDf = parallelRelDf.toPandas()\n",
    "    # grouped = pandasDf.groupby(partitionColName)\n",
    "    # # grouped.describe()\n",
    "    # for key in grouped.groups.keys():\n",
    "    #     segment = grouped.get_group(key)\n",
    "    #     segment.describe()\n",
    "    #     print(key + ': ' + str(len(segment.index)))    \n",
    "\n",
    "    # parallelRelDf.show()\n",
    "    writeDfToNeo(parallelRelDf, \"\"\"\n",
    "            MATCH(user:User {userId: event.userId})\n",
    "            MATCH(news:News {newsId: event.newsId})\n",
    "            MERGE(user)-[r:HISTORICALLY_CLICKED {splitSet: event.splitSet}]->(news)\n",
    "    \"\"\")\n",
    "\n",
    "    # writeDfToNeo(parallelRelDf, \"\"\"\n",
    "    #     MATCH(user:User {userId: event.userId})\n",
    "    #     MATCH(news:News {newsId: event.newsId})\n",
    "    #     MERGE(user)-[:DID_NOT_CLICK]->(news)\n",
    "    # \"\"\")\n",
    "\n",
    "    # writeDfToNeo(parallelRelDf, \"\"\"\n",
    "    #     MATCH(user:User {userId: event.userId})\n",
    "    #     MATCH(news:News {newsId: event.newsId})\n",
    "    #     MERGE(user)-[r:DID_NOT_CLICK {\n",
    "    #         splitSet: event.splitSet,\n",
    "    #         impressionId: event.impressionId,\n",
    "    #         impressionTime: event.time\n",
    "    #     }]->(news)\n",
    "    # \"\"\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Parallel Relationships Investigation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
